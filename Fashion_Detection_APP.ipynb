{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9MtA-_lp1Ce",
        "outputId": "2749fbe5-2265-467e-b199-56fa94fb720d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub tensorflow matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"paramaggarwal/fashion-product-images-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kf1fHwkp8uy",
        "outputId": "895ee82f-320c-4f5e-c39f-0841c5f81504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23.1G/23.1G [03:45<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8SJexyvzLUk",
        "outputId": "c5529263-27e9-4099-c3d6-15c042d7e042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/fashion-dataset\n"
      ],
      "metadata": {
        "id": "Z44_qSNJSEMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1/* fashion-dataset/\n"
      ],
      "metadata": {
        "id": "6EBx3QPoTFCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/fashion-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Dt5_lbTJ3v",
        "outputId": "f344720d-82f5-415f-d0ac-74c0b0847f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "t2RciULt7okb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the styles.csv file\n",
        "styles_csv_path = \"/content/fashion-dataset/fashion-dataset/fashion-dataset/styles.csv\"\n",
        "styles_data = pd.read_csv(styles_csv_path, on_bad_lines='skip')\n",
        "\n",
        "# Display the first few rows\n",
        "print(styles_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFgTFSvezoUm",
        "outputId": "a1ffe74f-f01a-4bb8-a083-b9cf0d2152fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
            "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
            "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
            "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
            "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
            "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
            "\n",
            "     year   usage                             productDisplayName  \n",
            "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
            "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
            "2  2016.0  Casual                       Titan Women Silver Watch  \n",
            "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
            "4  2012.0  Casual                          Puma Men Grey T-shirt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "jamebBqw7tpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect the images.csv file\n",
        "images_csv_path = \"/content/fashion-dataset/fashion-dataset/fashion-dataset/images.csv\"\n",
        "images_data = pd.read_csv(images_csv_path)\n",
        "\n",
        "# Display column names and preview the data\n",
        "print(\"Columns in images.csv:\", images_data.columns)\n",
        "print(images_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ET0F1RZUXgW",
        "outputId": "476a583f-3ec3-4314-e17e-b2712dec773f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in images.csv: Index(['filename', 'link'], dtype='object')\n",
            "    filename                                               link\n",
            "0  15970.jpg  http://assets.myntassets.com/v1/images/style/p...\n",
            "1  39386.jpg  http://assets.myntassets.com/v1/images/style/p...\n",
            "2  59263.jpg  http://assets.myntassets.com/v1/images/style/p...\n",
            "3  21379.jpg  http://assets.myntassets.com/v1/images/style/p...\n",
            "4  53759.jpg  http://assets.myntassets.com/v1/images/style/p...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract 'id' from 'filename' in images.csv\n",
        "images_data['id'] = images_data['filename'].str.extract(r'(\\d+)', expand=False).astype(int)\n",
        "\n",
        "# Display updated images_data\n",
        "print(images_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDSIkkVdUmhh",
        "outputId": "2883d875-5fbe-4ce3-e948-3f3b243ee560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    filename                                               link     id\n",
            "0  15970.jpg  http://assets.myntassets.com/v1/images/style/p...  15970\n",
            "1  39386.jpg  http://assets.myntassets.com/v1/images/style/p...  39386\n",
            "2  59263.jpg  http://assets.myntassets.com/v1/images/style/p...  59263\n",
            "3  21379.jpg  http://assets.myntassets.com/v1/images/style/p...  21379\n",
            "4  53759.jpg  http://assets.myntassets.com/v1/images/style/p...  53759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge images.csv and styles.csv on 'id'\n",
        "merged_data = pd.merge(styles_data, images_data, on='id', how='inner')\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(f\"Merged data: {len(merged_data)} rows\")\n",
        "print(merged_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbJxvy3eUt58",
        "outputId": "026e445f-feeb-44a7-c6c2-e7a656e0fe99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data: 44424 rows\n",
            "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
            "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
            "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
            "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
            "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
            "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
            "\n",
            "     year   usage                             productDisplayName   filename  \\\n",
            "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  15970.jpg   \n",
            "1  2012.0  Casual             Peter England Men Party Blue Jeans  39386.jpg   \n",
            "2  2016.0  Casual                       Titan Women Silver Watch  59263.jpg   \n",
            "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  21379.jpg   \n",
            "4  2012.0  Casual                          Puma Men Grey T-shirt  53759.jpg   \n",
            "\n",
            "                                                link  \n",
            "0  http://assets.myntassets.com/v1/images/style/p...  \n",
            "1  http://assets.myntassets.com/v1/images/style/p...  \n",
            "2  http://assets.myntassets.com/v1/images/style/p...  \n",
            "3  http://assets.myntassets.com/v1/images/style/p...  \n",
            "4  http://assets.myntassets.com/v1/images/style/p...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory for images\n",
        "image_dir = \"/content/fashion-dataset/fashion-dataset/fashion-dataset/images\"\n",
        "\n",
        "# Add full image paths\n",
        "merged_data['full_path'] = merged_data['filename'].apply(lambda x: os.path.join(image_dir, x))\n",
        "\n",
        "# Check if files exist\n",
        "merged_data['file_exists'] = merged_data['full_path'].apply(os.path.exists)\n",
        "\n",
        "# Filter valid rows with existing image files\n",
        "valid_data = merged_data[merged_data['file_exists']]\n",
        "\n",
        "print(f\"Number of valid images: {len(valid_data)}\")\n",
        "print(valid_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LizXA9agU1Qz",
        "outputId": "6383c77c-c341-4aaf-e345-7a890e8655ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid images: 44419\n",
            "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
            "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
            "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
            "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
            "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
            "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
            "\n",
            "     year   usage                             productDisplayName   filename  \\\n",
            "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  15970.jpg   \n",
            "1  2012.0  Casual             Peter England Men Party Blue Jeans  39386.jpg   \n",
            "2  2016.0  Casual                       Titan Women Silver Watch  59263.jpg   \n",
            "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  21379.jpg   \n",
            "4  2012.0  Casual                          Puma Men Grey T-shirt  53759.jpg   \n",
            "\n",
            "                                                link  \\\n",
            "0  http://assets.myntassets.com/v1/images/style/p...   \n",
            "1  http://assets.myntassets.com/v1/images/style/p...   \n",
            "2  http://assets.myntassets.com/v1/images/style/p...   \n",
            "3  http://assets.myntassets.com/v1/images/style/p...   \n",
            "4  http://assets.myntassets.com/v1/images/style/p...   \n",
            "\n",
            "                                           full_path  file_exists  \n",
            "0  /content/fashion-dataset/fashion-dataset/fashi...         True  \n",
            "1  /content/fashion-dataset/fashion-dataset/fashi...         True  \n",
            "2  /content/fashion-dataset/fashion-dataset/fashi...         True  \n",
            "3  /content/fashion-dataset/fashion-dataset/fashi...         True  \n",
            "4  /content/fashion-dataset/fashion-dataset/fashi...         True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Test Split"
      ],
      "metadata": {
        "id": "A2xEuN0L71WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select relevant columns\n",
        "data = valid_data[['full_path', 'subCategory']]\n",
        "\n",
        "# Train-validation split\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XmvPnrGU-jo",
        "outputId": "8aecfe25-3bef-4b2c-e536-1215ba34d4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 35535, Validation samples: 8884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels on the full dataset\n",
        "all_labels = data['subCategory']\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# Transform the labels for training and validation sets\n",
        "train_data['label'] = label_encoder.transform(train_data['subCategory'])\n",
        "val_data['label'] = label_encoder.transform(val_data['subCategory'])\n",
        "\n",
        "# Save the mapping for reference\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(f\"Label Mapping: {label_mapping}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o_O3s1hVFYX",
        "outputId": "41067572-fda4-4fba-ae07-ba96cb0fe70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping: {'Accessories': 0, 'Apparel Set': 1, 'Bags': 2, 'Bath and Body': 3, 'Beauty Accessories': 4, 'Belts': 5, 'Bottomwear': 6, 'Cufflinks': 7, 'Dress': 8, 'Eyes': 9, 'Eyewear': 10, 'Flip Flops': 11, 'Fragrance': 12, 'Free Gifts': 13, 'Gloves': 14, 'Hair': 15, 'Headwear': 16, 'Home Furnishing': 17, 'Innerwear': 18, 'Jewellery': 19, 'Lips': 20, 'Loungewear and Nightwear': 21, 'Makeup': 22, 'Mufflers': 23, 'Nails': 24, 'Perfumes': 25, 'Sandal': 26, 'Saree': 27, 'Scarves': 28, 'Shoe Accessories': 29, 'Shoes': 30, 'Skin': 31, 'Skin Care': 32, 'Socks': 33, 'Sports Accessories': 34, 'Sports Equipment': 35, 'Stoles': 36, 'Ties': 37, 'Topwear': 38, 'Umbrellas': 39, 'Vouchers': 40, 'Wallets': 41, 'Watches': 42, 'Water Bottle': 43, 'Wristbands': 44}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data sample:\")\n",
        "print(train_data.head())\n",
        "\n",
        "print(\"\\nValidation data sample:\")\n",
        "print(val_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alR-OfW-VAsi",
        "outputId": "ca358fbc-b21b-4612-91e8-2e35f75aa646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data sample:\n",
            "                                               full_path subCategory  label\n",
            "5041   /content/fashion-dataset/fashion-dataset/fashi...     Topwear     38\n",
            "36282  /content/fashion-dataset/fashion-dataset/fashi...     Topwear     38\n",
            "5882   /content/fashion-dataset/fashion-dataset/fashi...      Sandal     26\n",
            "8736   /content/fashion-dataset/fashion-dataset/fashi...        Bags      2\n",
            "5173   /content/fashion-dataset/fashion-dataset/fashi...  Bottomwear      6\n",
            "\n",
            "Validation data sample:\n",
            "                                               full_path subCategory  label\n",
            "41944  /content/fashion-dataset/fashion-dataset/fashi...     Eyewear     10\n",
            "24077  /content/fashion-dataset/fashion-dataset/fashi...     Watches     42\n",
            "39613  /content/fashion-dataset/fashion-dataset/fashi...     Topwear     38\n",
            "3150   /content/fashion-dataset/fashion-dataset/fashi...  Flip Flops     11\n",
            "44249  /content/fashion-dataset/fashion-dataset/fashi...  Flip Flops     11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Tensorflow"
      ],
      "metadata": {
        "id": "sN7JgFKA77LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define preprocessing function\n",
        "def load_image(image_path, label):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)  # Adjust for PNG if needed\n",
        "    img = tf.image.resize(img, [128, 128])  # Resize to desired dimensions\n",
        "    img = img / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img, label\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_data['full_path'], train_data['label']))\n",
        "train_ds = train_ds.map(load_image).batch(32).shuffle(buffer_size=1000)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_data['full_path'], val_data['label']))\n",
        "val_ds = val_ds.map(load_image).batch(32)\n",
        "\n",
        "print(\"TensorFlow datasets created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFSIVnpuVkgL",
        "outputId": "77ff95a6-1d90-43f1-d0f1-c10d05255f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow datasets created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model with an Input layer\n",
        "model = Sequential([\n",
        "    Input(shape=(128, 128, 3)),  # Specify input shape here\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(label_mapping), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MJsFl7ouVpRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "# Save the training history for future analysis\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ngwpXXR4gC",
        "outputId": "390e6467-1ed7-49aa-a1e4-cae6bac57a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m712s\u001b[0m 211ms/step - accuracy: 0.7989 - loss: 0.8148 - val_accuracy: 0.9068 - val_loss: 0.3510\n",
            "Epoch 2/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 197ms/step - accuracy: 0.9414 - loss: 0.2149 - val_accuracy: 0.9244 - val_loss: 0.2868\n",
            "Epoch 3/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 195ms/step - accuracy: 0.9658 - loss: 0.1104 - val_accuracy: 0.9346 - val_loss: 0.2981\n",
            "Epoch 4/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 189ms/step - accuracy: 0.9821 - loss: 0.0577 - val_accuracy: 0.9331 - val_loss: 0.3009\n",
            "Epoch 5/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 188ms/step - accuracy: 0.9896 - loss: 0.0358 - val_accuracy: 0.9287 - val_loss: 0.3758\n",
            "Epoch 6/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 251ms/step - accuracy: 0.9911 - loss: 0.0292 - val_accuracy: 0.9309 - val_loss: 0.4360\n",
            "Epoch 7/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 205ms/step - accuracy: 0.9924 - loss: 0.0256 - val_accuracy: 0.9347 - val_loss: 0.4025\n",
            "Epoch 8/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 249ms/step - accuracy: 0.9933 - loss: 0.0218 - val_accuracy: 0.9342 - val_loss: 0.4872\n",
            "Epoch 9/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m782s\u001b[0m 208ms/step - accuracy: 0.9949 - loss: 0.0193 - val_accuracy: 0.9351 - val_loss: 0.5073\n",
            "Epoch 10/10\n",
            "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 199ms/step - accuracy: 0.9965 - loss: 0.0126 - val_accuracy: 0.9320 - val_loss: 0.5463\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_accuracy = model.evaluate(val_ds)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXyhZnGvOPc",
        "outputId": "9a64eef0-b2ab-4ce5-b60d-4bfae296e9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 564ms/step - accuracy: 0.9313 - loss: 0.5530\n",
            "Validation Loss: 0.5463\n",
            "Validation Accuracy: 0.9320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in Keras format\n",
        "model.save('fashion_classifier.h5')\n",
        "print(\"Model saved as fashion_classifier.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPsx2DtqvUKF",
        "outputId": "93e54f08-2b89-497a-f21e-a4deca163736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as fashion_classifier.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open('fashion_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model saved as fashion_classifier.tflite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlBC3CEWvWh7",
        "outputId": "6e78c5ee-331b-4f23-d074-2526ec1ddc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmphkhnw4__'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 45), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136568024110304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568023659120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015831632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015831280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015846592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015846944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015832336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136568015405120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model saved as fashion_classifier.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of MOdel"
      ],
      "metadata": {
        "id": "4hsm2xZ202Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "X1qvcaJV01o-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your .tflite model\n",
        "model_path = \"/content/fashion_classifier.tflite\"\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input Details:\", input_details)\n",
        "print(\"Output Details:\", output_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "988BJEqF1Ctl",
        "outputId": "b5873f47-432f-49fe-e895-4b2fbe4f7302"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Details: [{'name': 'serving_default_keras_tensor:0', 'index': 0, 'shape': array([  1, 128, 128,   3], dtype=int32), 'shape_signature': array([ -1, 128, 128,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output Details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 17, 'shape': array([ 1, 45], dtype=int32), 'shape_signature': array([-1, 45], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HxPW2L5d1KJ7",
        "outputId": "f4195dba-336b-4065-e02f-02de9c36f0c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6727b7c6-8046-423b-9e1f-90516abd789e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6727b7c6-8046-423b-9e1f-90516abd789e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 10004.jpg to 10004.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/10004.jpg\"  # Replace with your image name\n",
        "image = Image.open(image_path).resize((128, 128))  # Resize to 128x128\n",
        "\n",
        "# Convert to numpy array and normalize\n",
        "input_image = np.array(image).astype(np.float32) / 255.0\n",
        "input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension\n"
      ],
      "metadata": {
        "id": "wI7ikHVU1cfo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "\n",
        "# Run the model\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Print output (confidence scores)\n",
        "print(\"Model Output:\", output_data)\n",
        "\n",
        "# Find the predicted class\n",
        "predicted_class = np.argmax(output_data)\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf6fYQvy1jpM",
        "outputId": "e97a3636-8320-4929-9c29-5bf3c6481c1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: [[1.07654665e-20 2.30723908e-23 3.36870992e-17 3.17822689e-28\n",
            "  3.63732128e-24 1.84342822e-20 1.00000000e+00 4.21163371e-23\n",
            "  7.81402628e-21 1.74176788e-16 1.37280509e-23 9.75474195e-27\n",
            "  1.58863957e-17 3.11172277e-17 4.45801745e-17 8.72685886e-21\n",
            "  8.45022564e-23 9.45690754e-31 1.81201507e-17 2.84445079e-19\n",
            "  1.06444163e-15 1.44209926e-12 3.55715933e-16 6.62865010e-19\n",
            "  2.47442947e-22 8.89223503e-27 7.33885110e-24 3.79228756e-23\n",
            "  3.30145683e-20 6.24277893e-21 3.66615646e-17 2.39366349e-16\n",
            "  5.32220511e-20 3.35247107e-15 7.67965962e-28 1.42499726e-26\n",
            "  4.29837458e-22 1.94128445e-21 1.01030186e-17 2.18969019e-30\n",
            "  5.39244956e-33 2.52480060e-16 1.45671371e-14 3.16463995e-21\n",
            "  4.12067349e-28]]\n",
            "Predicted Class: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output Shape:\", output_data.shape)\n",
        "print(\"Output Data:\", output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwnOEe-11omG",
        "outputId": "78eb8a99-da77-4492-c7ab-ce88847faed2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: (1, 45)\n",
            "Output Data: [[1.07654665e-20 2.30723908e-23 3.36870992e-17 3.17822689e-28\n",
            "  3.63732128e-24 1.84342822e-20 1.00000000e+00 4.21163371e-23\n",
            "  7.81402628e-21 1.74176788e-16 1.37280509e-23 9.75474195e-27\n",
            "  1.58863957e-17 3.11172277e-17 4.45801745e-17 8.72685886e-21\n",
            "  8.45022564e-23 9.45690754e-31 1.81201507e-17 2.84445079e-19\n",
            "  1.06444163e-15 1.44209926e-12 3.55715933e-16 6.62865010e-19\n",
            "  2.47442947e-22 8.89223503e-27 7.33885110e-24 3.79228756e-23\n",
            "  3.30145683e-20 6.24277893e-21 3.66615646e-17 2.39366349e-16\n",
            "  5.32220511e-20 3.35247107e-15 7.67965962e-28 1.42499726e-26\n",
            "  4.29837458e-22 1.94128445e-21 1.01030186e-17 2.18969019e-30\n",
            "  5.39244956e-33 2.52480060e-16 1.45671371e-14 3.16463995e-21\n",
            "  4.12067349e-28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = np.argmax(output_data)\n",
        "print(\"Predicted Class Index:\", predicted_class)\n",
        "\n",
        "if predicted_class < len(class_labels):\n",
        "    print(\"Predicted Label:\", class_labels[predicted_class])\n",
        "else:\n",
        "    print(f\"Error: Predicted class index ({predicted_class}) is out of range.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvqJuyFo2CsY",
        "outputId": "81c50163-8a44-4287-9e01-de657ac47de9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class Index: 6\n",
            "Error: Predicted class index (6) is out of range.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Path to your TensorFlow Lite model\n",
        "model_path = \"/content/fashion_classifier.tflite\"\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Model Input Details:\", input_details)\n",
        "print(\"Model Output Details:\", output_details)\n",
        "\n",
        "# Correct class labels from your label mapping\n",
        "class_labels = [\n",
        "    'Accessories', 'Apparel Set', 'Bags', 'Bath and Body', 'Beauty Accessories',\n",
        "    'Belts', 'Bottomwear', 'Cufflinks', 'Dress', 'Eyes', 'Eyewear', 'Flip Flops',\n",
        "    'Fragrance', 'Free Gifts', 'Gloves', 'Hair', 'Headwear', 'Home Furnishing',\n",
        "    'Innerwear', 'Jewellery', 'Lips', 'Loungewear and Nightwear', 'Makeup',\n",
        "    'Mufflers', 'Nails', 'Perfumes', 'Sandal', 'Saree', 'Scarves',\n",
        "    'Shoe Accessories', 'Shoes', 'Skin', 'Skin Care', 'Socks', 'Sports Accessories',\n",
        "    'Sports Equipment', 'Stoles', 'Ties', 'Topwear', 'Umbrellas', 'Vouchers',\n",
        "    'Wallets', 'Watches', 'Water Bottle', 'Wristbands'\n",
        "]\n",
        "\n",
        "# Single upload and classify\n",
        "while True:\n",
        "    print(\"Upload an image to classify (or type 'stop' to end):\")\n",
        "    uploaded = files.upload()  # Upload an image\n",
        "\n",
        "    # Stop if no file is uploaded or the user types 'stop'\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Get the uploaded image file path\n",
        "    image_path = list(uploaded.keys())[0]\n",
        "    print(f\"Classifying image: {image_path}\")\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).resize((128, 128))  # Resize to match model input\n",
        "    input_image = np.array(image).astype(np.float32) / 255.0  # Normalize pixel values\n",
        "    input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Set the input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Print confidence scores\n",
        "    print(\"Confidence Scores:\", output_data)\n",
        "\n",
        "    # Find the predicted class\n",
        "    predicted_class = np.argmax(output_data)\n",
        "    print(\"Predicted Class Index:\", predicted_class)\n",
        "\n",
        "    # Ensure the predicted class index is valid\n",
        "    if predicted_class < len(class_labels):\n",
        "        predicted_label = class_labels[predicted_class]\n",
        "        print(f\"Predicted Label: {predicted_label}\")\n",
        "    else:\n",
        "        print(f\"Error: Predicted class index ({predicted_class}) is out of range.\")\n",
        "\n",
        "    print(\"-\" * 50)  # Separator for next run\n",
        "    break  # Stop after processing the first file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "nFopGJC56qgw",
        "outputId": "ce720f84-106c-4b40-8fc4-75d93f752c74"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Input Details: [{'name': 'serving_default_keras_tensor:0', 'index': 0, 'shape': array([  1, 128, 128,   3], dtype=int32), 'shape_signature': array([ -1, 128, 128,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Model Output Details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 17, 'shape': array([ 1, 45], dtype=int32), 'shape_signature': array([-1, 45], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Upload an image to classify (or type 'stop' to end):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bff32e41-d0f0-40e6-ba04-f07ddc38b21f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bff32e41-d0f0-40e6-ba04-f07ddc38b21f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving download (4).jpeg to download (4).jpeg\n",
            "Classifying image: download (4).jpeg\n",
            "Confidence Scores: [[2.1333010e-03 1.8490411e-01 2.2669262e-01 1.1797115e-05 4.3991677e-06\n",
            "  3.5264794e-10 2.7757462e-02 3.6425900e-09 8.8166066e-02 9.7878173e-04\n",
            "  9.5079253e-07 6.5646304e-08 2.2198159e-04 2.5281666e-02 1.8004463e-13\n",
            "  2.9805386e-05 1.7879665e-06 5.2112193e-11 2.1672423e-07 1.9428082e-06\n",
            "  2.2966717e-05 5.5759139e-03 1.0640382e-02 5.5643231e-05 1.7724724e-05\n",
            "  9.9272626e-08 1.5795425e-05 9.2261347e-05 1.6655904e-04 1.0638660e-03\n",
            "  7.5545181e-06 2.0887987e-03 3.7662074e-04 1.7704610e-03 2.3526833e-07\n",
            "  6.2622767e-06 3.7039528e-04 5.2874483e-04 9.4222516e-02 3.9500745e-09\n",
            "  9.9563930e-09 2.6195539e-07 5.7064623e-02 2.6972443e-01 8.9174171e-07]]\n",
            "Predicted Class Index: 43\n",
            "Predicted Label: Water Bottle\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}